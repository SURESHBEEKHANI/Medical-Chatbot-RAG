{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydantic_core\n",
    "from langchain_core._api import deprecation\n",
    "print(\"✅ pydantic-core & LangChain core OK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# Imports\n",
    "# ===============================\n",
    "from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import os\n",
    "\n",
    "# ===============================\n",
    "# Function to load PDFs\n",
    "# ===============================\n",
    "def load_pdf_files(directory_path: str):\n",
    "    \"\"\"\n",
    "    Loads all PDF files from the given directory and returns a list of documents.\n",
    "\n",
    "    Args:\n",
    "        directory_path (str): Path to the directory containing PDFs.\n",
    "\n",
    "    Returns:\n",
    "        List[Document]: A list of LangChain Document objects.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(directory_path):\n",
    "        raise FileNotFoundError(f\"Directory not found: {directory_path}\")\n",
    "\n",
    "    loader = DirectoryLoader(\n",
    "        directory_path,\n",
    "        glob=\"*.pdf\",\n",
    "        loader_cls=PyPDFLoader\n",
    "    )\n",
    "    documents = loader.load()\n",
    "    return documents\n",
    "\n",
    "# ===============================\n",
    "# Example usage\n",
    "# ===============================\n",
    "try:\n",
    "    extracted_data = load_pdf_files(\"data\")\n",
    "    print(f\"✅ Loaded {len(extracted_data)} PDF documents successfully.\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"❌ {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ An error occurred while loading PDFs: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the Data into Text Chunks\n",
    "def text_split(extracted_data):\n",
    "    text_splitter=RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
    "    text_chunks=text_splitter.split_documents(extracted_data)\n",
    "    return text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunks=text_split(extracted_data)\n",
    "print(\"Length of Text Chunks\", len(text_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download the Embeddings from Hugging Face\n",
    "def download_hugging_face_embeddings():\n",
    "    embeddings=HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = download_hugging_face_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_result = embeddings.embed_query(\"Hello world\")\n",
    "print(\"Length\", len(query_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PINECONE_API_KEY=os.environ.get('PINECONE_API_KEY')\n",
    "groq_API_KEY=os.environ.get('groq_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")  # Use env variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import ServerlessSpec,Pinecone\n",
    "\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "index_name = \"medical-chatbot\"\n",
    "\n",
    "if not pc.has_index(index_name):\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=384,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
    "    )\n",
    "\n",
    "index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "\n",
    "\n",
    "vector_store = PineconeVectorStore(index=index_name=, embedding=embeddings)\n",
    "\n",
    "# Embed each chunk and upsert the embeddings into your Pinecone index.\n",
    "\n",
    "# Upsert your chunked documents\n",
    "vector_store.add_documents(text_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "# Create the vector store from an existing Pinecone index\n",
    "docsearch = PineconeVectorStore.from_existing_index(\n",
    "    index_name=index_name,\n",
    "    embedding=embeddings\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve documents for the query\n",
    "retrieved_docs = retriever.invoke(\"What is Acne?\")\n",
    "\n",
    "# Combine all retrieved document contents into a single context string\n",
    "context_text = \"\\n\".join(doc.page_content for doc in retrieved_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# Initialize the Groq chat model\n",
    "llm = ChatGroq(\n",
    "    model=\"openai/gpt-oss-20b\",  # model name\n",
    "    temperature=0.0,             # deterministic responses\n",
    "    max_tokens=100              # optional: set max tokens per response\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.invoke(\"What is Acne?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Define the system prompt\n",
    "# -------------------------------\n",
    "system_prompt = (\n",
    "    \"You are a question answering assistant. \"\n",
    "    \"Use the retrieved context below. \"\n",
    "    \"If you don't know the answer, say you don't know. \"\n",
    "    \"Be concise (3 sentences max).\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Create the chat prompt\n",
    "# -------------------------------\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),  # Must match the key in invoke()\n",
    "    ]\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Initialize output parser\n",
    "# -------------------------------\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Define the RAG chain\n",
    "# -------------------------------\n",
    "def rag_chain(question):\n",
    "    # Retrieve relevant docs\n",
    "    docs = retriever.invoke(question)\n",
    "    context = \"\\n\\n\".join([d.page_content for d in docs])\n",
    "\n",
    "    # Chain: prompt -> LLM -> output parser\n",
    "    chain = prompt | llm | output_parser\n",
    "\n",
    "    # Invoke the chain with proper variable names\n",
    "    return chain.invoke({\n",
    "        \"context\": context,\n",
    "        \"input\": question  # must match {input} in human message\n",
    "    })\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Test\n",
    "# -------------------------------\n",
    "response = rag_chain(\"What is acne?\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jupyter nbconvert --clear-output --inplace backend/notebook/trials.ipynb\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "backend",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
